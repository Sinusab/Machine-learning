import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
data = pd.read_csv('project3data1.csv')
X = data.values[: , 0 : 2]
Y = data.values[: , 2]
m = X.shape[0]

def plotData(x , y):
    pos = (y == 1)
    neg = (y == 0)
    plt.plot(x[pos , 0] , x[pos , 1] , 'r*' , ms = 8)
    plt.plot(x[neg , 0] , x[neg , 1] , 'bo' , ms = 8)
    plt.legend(['passed', 'not passed'])

plotData(X,Y)

X_train = np.copy(X)
Y = Y.reshape(1,-1)
X_train = np.concatenate((np.ones((1,m)) , X.T) , axis = 0)
theta = np.zeros((X_train.shape[0], 1))

def sigmoid(x):
    return 1 / (1 + np.exp(-x))
def costFunction(x , y , theta):
    m = x.shape[1]
    h = sigmoid(np.matmul(theta.T, x))
    h = np.clip(h, 1e-10, 1 - 1e-10)
    cost = (-1/m) *(np.matmul(np.log(h), y.T) + np.matmul(np.log (1 - h) , (1 - y).T))
    return cost

costFunction(X_train , Y , theta)

def gradientDescent(x , y , theta , lr , epochs):
    m = x.shape[1]
    costs = []
    for epoch in range(epochs):
        cost = costFunction(x , y , theta)
        costs.append(cost)
        h = sigmoid(np.matmul(theta.T , x))
        theta = theta - (lr/m) * np.matmul(x , (h - y).T)
    return theta , costs

theta = np.zeros((X_train.shape[0], 1))
theta, J_history = gradientDescent(X_train, Y, theta, 0.004, 300000)
J_history[-1]

theta

def plotDecisionBoundary(plotData, theta, X, y):
    
    #make sure theta is a numpy array
    theta = np.array(theta)
    
    
    #Plot Data (remember first column in X is the intercept)
    plotData(X, y)

    if X.shape[1] <= 3:
        # Only need 2 points to define a line, so choose two endpoints
        plot_x = np.array([np.min(X[:, 1]) - 2, np.max(X[:, 1]) + 2])

        # Calculate the decision boundary line
        plot_y = (-1. / theta[2]) * (theta[1] * plot_x + theta[0])
        
        # Plot, and adjust axes for better viewing
        plt.plot(plot_x, plot_y)

        # Legend, specific for the exercise
        plt.legend(['Passed', 'Not Passed', 'Decision Boundary'])
        plt.xlim([30, 100])
        plt.ylim([30, 100])
    else:
        # Here is the grid range
        u = np.linspace(-1, 1.5, 50)
        v = np.linspace(-1, 1.5, 50)

        z = np.zeros((u.size, v.size))
        # Evaluate z = theta*x over the grid
        for i, ui in enumerate(u):
            for j, vj in enumerate(v):
                z[i, j] = np.dot(mapFeature(np.array([ui, vj], ndmin=2)), theta)

        z = z.T  # important to transpose z before calling contour
        # print(z)

        # Plot z = 0
        plt.contour(u, v, z, levels=[0], linewidths=2, colors='g')
        plt.contourf(u, v, z, levels=[np.min(z), 0, np.max(z)], cmap='Greens', alpha=0.4)


X_new = X_train[1: , :].T
Y_new = Y[0 , :]

plotDecisionBoundary(plotData , theta , X_new , Y_new)

def prediction(x):
    x = x.reshape(1,-1)
    x = np.concatenate((np.ones((1,1)) , x.T) , axis = 0)    
    a = sigmoid (np.matmul(theta.T , x))
    grade = True if a >= 0.5 else False
    return a , grade

wrong_predictions = []
for i in range(100):
    x = X[i, :]
    _, predicted_class = prediction(x)
    if predicted_class != Y[0, i]:
        a.append(i)

for item in a:
    print(f'{prediction (X[item , :])} and this is the {item}th student')
